{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239be6af",
   "metadata": {},
   "source": [
    "### P02 - Batching\n",
    "\n",
    "This notebook will cover:\n",
    "- What is batching?\n",
    "- Throughput vs latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd87319-4656-45f1-a6be-0a0b970e01f9",
   "metadata": {},
   "source": [
    "#### Import required packages and load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ea434a9-22e6-4bee-8ef3-65e8992ce089",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7066fad6-94df-4153-a6de-a33d7dace6b4",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "model_name = \"./models/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112045ae-5e41-4a11-b95e-79961893853b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of parameters in the model: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Get the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"The number of parameters in the model: {num_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca61e7-ad31-49ae-9338-6e38b0881dde",
   "metadata": {},
   "source": [
    "#### KV-cache setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b79f19-38f6-4f34-804d-921ceb40931b",
   "metadata": {
    "height": 625
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumped over the -  fence and ran to the other side of the fence\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The quick brown fox jumped over the\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "def generate_token_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[0, -1, :]\n",
    "    next_token_id = last_logits.argmax()\n",
    "    return next_token_id, outputs.past_key_values\n",
    "\n",
    "\n",
    "def generate(inputs, max_tokens):\n",
    "    generated_tokens = []\n",
    "    next_inputs = inputs\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_id, past_key_values = generate_token_with_past(next_inputs)\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_id.reshape((1, 1)),\n",
    "            \"attention_mask\": torch.cat(\n",
    "                [next_inputs[\"attention_mask\"], torch.tensor([[1]], device=\"cuda\")],\n",
    "                dim=1\n",
    "            ),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "        next_token = tokenizer.decode(next_token_id)\n",
    "        generated_tokens.append(next_token)\n",
    "    return \"\".join(generated_tokens)\n",
    "\n",
    "\n",
    "tokens = generate(inputs, max_tokens=10)\n",
    "print(f\"{prompt} - {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ad913-0c0e-450b-afa0-ff289b85b5f1",
   "metadata": {},
   "source": [
    "#### Add padding tokens to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d895c2e9-fd7f-41d9-af00-a44a19d512b2",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e09245a5-d032-4a45-a2e8-dad82305b449",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# pad on the left so we can append new tokens on the right\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea8d58c-3732-4b00-9819-9d3c45e4fd04",
   "metadata": {},
   "source": [
    "Add padding to a batch of prompts such that they have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17dc969e-eaf1-4594-b328-2fbb8dbd470a",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: tensor([[  464,  2068,  7586, 21831, 11687,   625,   262],\n",
      "        [50256, 50256,   464,  6290,   287,  8602,  8953],\n",
      "        [50256, 50256, 50256,  2061,  2058,   510,  1276]], device='cuda:0')\n",
      "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"The quick brown fox jumped over the\",\n",
    "    \"The rain in Spain falls\",\n",
    "    \"What comes up must\",\n",
    "]\n",
    "inputs = tokenizer(prompts, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "print(\"input_ids:\", inputs[\"input_ids\"])\n",
    "print(\"attention_mask:\", inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264a70b-d66c-439a-873b-732579eb71c3",
   "metadata": {},
   "source": [
    "Add position ids to track original order of tokens in each prompt; padding tokens are set to `1` and then first real token starts with position `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9fd5ca-7bc0-48ad-a6f4-88c29598570c",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6],\n",
      "        [1, 1, 0, 1, 2, 3, 4],\n",
      "        [1, 1, 1, 0, 1, 2, 3]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# position_ids tell the transformer the ordinal position of each token in the input sequence\n",
    "# for single input inference, this is just [0 .. n] for n tokens\n",
    "# but for batch inference, we need to zero-out the padding tokens at the start of the sequence\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "position_ids.masked_fill_(attention_mask == 0, 1).to(\"cuda\")\n",
    "print(position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117ed42c-cc0b-4ca8-b656-dd1484fc5574",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 7, 50257])\n"
     ]
    }
   ],
   "source": [
    "# pass tokens to model to calculate logits\n",
    "# same as before, but include the position_ids\n",
    "with torch.no_grad():\n",
    "    outputs = model(position_ids=position_ids, **inputs)\n",
    "logits = outputs.logits\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a70df683-bc0a-403b-9cbf-014cb2362f01",
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13990,   319,   307], device='cuda:0')\n",
      "[' fence', ' on', ' be']\n"
     ]
    }
   ],
   "source": [
    "# retrieve mostly likely token for each prompt\n",
    "last_logits = logits[:, -1, :] \n",
    "next_token_ids = last_logits.argmax(dim=1) \n",
    "print(next_token_ids)\n",
    "next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "print(next_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad750c23",
   "metadata": {},
   "source": [
    "#### Put it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3543bb0-2a33-49dc-9d61-5af911a03d24",
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "# generate n tokens for each prompt - no change\n",
    "def generate_batch_tokens_with_past(inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    last_logits = logits[:, -1, :]\n",
    "    next_token_ids = last_logits.argmax(dim=1)\n",
    "    return next_token_ids, outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb50b1e6-415f-4315-8816-8818df516a07",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "# generate max_tokens for each prompt\n",
    "def generate_batch(inputs, max_tokens):\n",
    "    # create a list of tokens for every input in the batch\n",
    "    generated_tokens = [\n",
    "        [] for _ in range(inputs[\"input_ids\"].shape[0])\n",
    "    ]\n",
    "\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "    position_ids.masked_fill_(attention_mask == 0, 1).to(\"cuda\")\n",
    "\n",
    "    next_inputs = {\n",
    "        \"position_ids\": position_ids,\n",
    "        **inputs\n",
    "    }\n",
    "\n",
    "    for _ in range(max_tokens):\n",
    "        next_token_ids, past_key_values = generate_batch_tokens_with_past(next_inputs)\n",
    "\n",
    "        next_inputs = {\n",
    "            \"input_ids\": next_token_ids.reshape((-1, 1)), # reshape from (batch_size,) to (batch_size, 1)\n",
    "            \"position_ids\": next_inputs[\"position_ids\"][:, -1].unsqueeze(-1) + 1, # slice as (batch_size, ) and unsqueeze to (batch_size, 1)\n",
    "            \"attention_mask\": torch.cat([\n",
    "                next_inputs[\"attention_mask\"],\n",
    "                torch.ones((next_token_ids.shape[0], 1), device=\"cuda\"),\n",
    "            ], dim=1),\n",
    "            \"past_key_values\": past_key_values,\n",
    "        }\n",
    "\n",
    "        next_tokens = tokenizer.batch_decode(next_token_ids)\n",
    "        for i, token in enumerate(next_tokens):\n",
    "            generated_tokens[i].append(token)\n",
    "    return [\"\".join(tokens) for tokens in generated_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bd8d5cc-c06c-4efc-97da-ebcd3b2e15b6",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumped over the \u001b[31m fence and ran to the other side of the fence\u001b[0m\n",
      "--------\n",
      "The rain in Spain falls \u001b[31m on the first day of the month, and the\u001b[0m\n",
      "--------\n",
      "What comes up must \u001b[31m be a good idea.\n",
      "\n",
      "\"I think\u001b[0m\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# generate 10 tokens for each prompt\n",
    "generated_tokens = generate_batch(inputs, max_tokens=10)\n",
    "\n",
    "for prompt, generated in zip(prompts, generated_tokens):\n",
    "    print(prompt, f\"\\x1b[31m{generated}\\x1b[0m\")\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077e2c0",
   "metadata": {},
   "source": [
    "#### Throughput vs Latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35e3a944-e2f3-4cda-a5e3-d753b223e73a",
   "metadata": {
    "height": 591
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs=   1, duration= 59.8ms, throughput=  0.2 tokens/ms, avg latency= 6.0ms\n",
      "bs=   2, duration= 59.9ms, throughput=  0.3 tokens/ms, avg latency= 6.0ms\n",
      "bs=   4, duration= 59.3ms, throughput=  0.7 tokens/ms, avg latency= 5.9ms\n",
      "bs=   8, duration= 61.6ms, throughput=  1.3 tokens/ms, avg latency= 6.2ms\n",
      "bs=  16, duration= 64.4ms, throughput=  2.5 tokens/ms, avg latency= 6.4ms\n",
      "bs=  32, duration= 67.2ms, throughput=  4.8 tokens/ms, avg latency= 6.7ms\n",
      "bs=  64, duration= 75.6ms, throughput=  8.5 tokens/ms, avg latency= 7.6ms\n",
      "bs= 128, duration= 93.0ms, throughput= 13.8 tokens/ms, avg latency= 9.3ms\n"
     ]
    }
   ],
   "source": [
    "# constants\n",
    "max_tokens = 10\n",
    "\n",
    "# observations\n",
    "durations = []\n",
    "throughputs = []\n",
    "latencies = []\n",
    "\n",
    "batch_sizes = [2**p for p in range(8)]\n",
    "for batch_size in batch_sizes:\n",
    "    # generate tokens for batch and record duration\n",
    "    t0 = time.time()\n",
    "    batch_prompts = [prompts[i % len(prompts)] for i in range(batch_size)]\n",
    "    inputs = tokenizer(batch_prompts, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_tokens = generate_batch(inputs, max_tokens=max_tokens)\n",
    "    duration_ms = (time.time() - t0) * 1e3\n",
    "\n",
    "    ntokens = batch_size * max_tokens\n",
    "    throughput = ntokens / duration_ms\n",
    "    avg_latency = duration_ms / max_tokens\n",
    "\n",
    "    print(f\"bs= {batch_size:3d}, duration= {duration_ms:.1f}ms, throughput= {throughput:4.1f} tokens/ms, avg latency= {avg_latency:3.1f}ms\")\n",
    "\n",
    "    durations.append(duration_ms)\n",
    "    throughputs.append(throughput)\n",
    "    latencies.append(avg_latency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
